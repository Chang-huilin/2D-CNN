{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Using device: cuda\n",
      "Epoch 1/50 | Loss: 0.8863 | Val Acc: 31.06%\n",
      "Epoch 2/50 | Loss: 0.7187 | Val Acc: 49.50%\n",
      "Epoch 3/50 | Loss: 0.6853 | Val Acc: 67.13%\n",
      "Epoch 4/50 | Loss: 0.6368 | Val Acc: 39.68%\n",
      "Epoch 5/50 | Loss: 0.5389 | Val Acc: 47.70%\n",
      "Epoch 6/50 | Loss: 0.5415 | Val Acc: 49.50%\n",
      "Epoch 7/50 | Loss: 0.5354 | Val Acc: 48.90%\n",
      "Epoch 8/50 | Loss: 0.4495 | Val Acc: 65.33%\n",
      "Epoch 9/50 | Loss: 0.4067 | Val Acc: 38.28%\n",
      "Epoch 10/50 | Loss: 0.3529 | Val Acc: 44.29%\n",
      "Epoch 11/50 | Loss: 0.3105 | Val Acc: 33.67%\n",
      "Epoch 12/50 | Loss: 0.3194 | Val Acc: 40.48%\n",
      "Epoch 13/50 | Loss: 0.2412 | Val Acc: 75.55%\n",
      "Epoch 14/50 | Loss: 0.2649 | Val Acc: 31.06%\n",
      "Epoch 15/50 | Loss: 0.2072 | Val Acc: 74.15%\n",
      "Epoch 16/50 | Loss: 0.1925 | Val Acc: 74.35%\n",
      "Epoch 17/50 | Loss: 0.1445 | Val Acc: 67.54%\n",
      "Epoch 18/50 | Loss: 0.1986 | Val Acc: 56.31%\n",
      "Epoch 19/50 | Loss: 0.1258 | Val Acc: 56.11%\n",
      "Epoch 20/50 | Loss: 0.1754 | Val Acc: 33.27%\n",
      "Epoch 21/50 | Loss: 0.1332 | Val Acc: 54.51%\n",
      "Epoch 22/50 | Loss: 0.0959 | Val Acc: 55.91%\n",
      "Epoch 23/50 | Loss: 0.1308 | Val Acc: 39.68%\n",
      "Epoch 24/50 | Loss: 0.1155 | Val Acc: 67.13%\n",
      "Epoch 25/50 | Loss: 0.0920 | Val Acc: 51.30%\n",
      "Epoch 26/50 | Loss: 0.1113 | Val Acc: 70.34%\n",
      "Epoch 27/50 | Loss: 0.0579 | Val Acc: 74.75%\n",
      "Epoch 28/50 | Loss: 0.0659 | Val Acc: 53.11%\n",
      "Epoch 29/50 | Loss: 0.0861 | Val Acc: 71.14%\n",
      "Epoch 30/50 | Loss: 0.0688 | Val Acc: 54.31%\n",
      "Epoch 31/50 | Loss: 0.0598 | Val Acc: 72.75%\n",
      "Epoch 32/50 | Loss: 0.0646 | Val Acc: 76.55%\n",
      "Epoch 33/50 | Loss: 0.0611 | Val Acc: 72.55%\n",
      "Epoch 34/50 | Loss: 0.0782 | Val Acc: 60.52%\n",
      "Epoch 35/50 | Loss: 0.0798 | Val Acc: 36.07%\n",
      "Epoch 36/50 | Loss: 0.0542 | Val Acc: 72.34%\n",
      "Epoch 37/50 | Loss: 0.0188 | Val Acc: 58.32%\n",
      "Epoch 38/50 | Loss: 0.0590 | Val Acc: 71.94%\n",
      "Epoch 39/50 | Loss: 0.0888 | Val Acc: 64.93%\n",
      "Epoch 40/50 | Loss: 0.0276 | Val Acc: 57.11%\n",
      "Epoch 41/50 | Loss: 0.0258 | Val Acc: 70.34%\n",
      "Epoch 42/50 | Loss: 0.0431 | Val Acc: 76.95%\n",
      "Epoch 43/50 | Loss: 0.0709 | Val Acc: 71.74%\n",
      "Epoch 44/50 | Loss: 0.1001 | Val Acc: 46.29%\n",
      "Epoch 45/50 | Loss: 0.0605 | Val Acc: 42.48%\n",
      "Epoch 46/50 | Loss: 0.0460 | Val Acc: 41.68%\n",
      "Epoch 47/50 | Loss: 0.0921 | Val Acc: 64.33%\n",
      "Epoch 48/50 | Loss: 0.0778 | Val Acc: 78.96%\n",
      "Epoch 49/50 | Loss: 0.0229 | Val Acc: 75.75%\n",
      "Epoch 50/50 | Loss: 0.0288 | Val Acc: 44.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enine\\AppData\\Local\\Temp\\ipykernel_33940\\2121516164.py:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 77.80%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# 强制使用 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 自定义数据集类\n",
    "class EnhancedDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
    "        self.images = self._load_images()\n",
    "\n",
    "    def _load_images(self):\n",
    "        images = []\n",
    "        for cls in self.classes:\n",
    "            cls_dir = os.path.join(self.root_dir, cls)\n",
    "            for img_name in os.listdir(cls_dir):\n",
    "                img_path = os.path.join(cls_dir, img_name)\n",
    "                images.append((img_path, self.class_to_idx[cls]))\n",
    "        return images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.images[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# CBAM模块定义\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super().__init__()\n",
    "        self.maxpool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.se = nn.Sequential(\n",
    "            nn.Conv2d(channel, channel // reduction, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channel // reduction, channel, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        max_result = self.maxpool(x)\n",
    "        avg_result = self.avgpool(x)\n",
    "        max_out = self.se(max_result)\n",
    "        avg_out = self.se(avg_result)\n",
    "        output = self.sigmoid(max_out + avg_out)\n",
    "        return output\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        max_result, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        avg_result = torch.mean(x, dim=1, keepdim=True)\n",
    "        result = torch.cat([max_result, avg_result], 1)\n",
    "        output = self.conv(result)\n",
    "        output = self.sigmoid(output)\n",
    "        return output\n",
    "\n",
    "class CBAMBlock(nn.Module):\n",
    "    def __init__(self, channel=512, reduction=16, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.ca = ChannelAttention(channel=channel, reduction=reduction)\n",
    "        self.sa = SpatialAttention(kernel_size=kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x * self.ca(x)\n",
    "        out = out * self.sa(out)\n",
    "        return out\n",
    "\n",
    "# 带CBAM的分类模型\n",
    "class CBAMClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=3):  # \n",
    "        super().__init__()\n",
    "        \n",
    "        # 基础卷积模块\n",
    "        self.base = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),  #\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            CBAMBlock(channel=64, reduction=16),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            CBAMBlock(channel=128, reduction=16),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        \n",
    "        # 分类器\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "# 训练配置\n",
    "def train_model():\n",
    "    # 检查GPU是否可用\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # 数据预处理\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    # 加载数据集\n",
    "    dataset = EnhancedDataset(\n",
    "        root_dir=r\"L:\\常惠林\\萎凋\\自然萎凋\\分类\",\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    # 按6:2:2划分训练集、验证集和测试集\n",
    "    train_size = int(0.6 * len(dataset))\n",
    "    val_size = int(0.2 * len(dataset))\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "    # 初始化模型并移动到GPU\n",
    "    model = CBAMClassifier(num_classes=len(dataset.classes)).to(device)\n",
    "\n",
    "    # 损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "\n",
    "    # 训练循环\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # 数据移动到GPU\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)  # 数据移动到GPU\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_acc = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}/50 | Loss: {running_loss/len(train_loader):.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        # 保存最佳模型\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "    # 测试阶段\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # 数据移动到GPU\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_acc = 100 * test_correct / test_total\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "42",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
